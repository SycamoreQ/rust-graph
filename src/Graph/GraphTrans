mod GraphTrans;

use crate::Graph::graph::{Node , Edge , Graph , GraphOps , EdgeDirection , NodeID , EdgeID};
use crate::attn::{CrossAttention , BasicTransformerBlock , AttentionBlock};
use candle::core::{Tensor, Device, DType};
use candle::nn::ops::{softmax , layer_norm , dropout , softmax_last_dim , leaky_relu};
use candle::nn::init::Init;
use candle::candle_transformers as transformers; 


pub type embedding_dim = usize;
pub type input_dim = usize;
pub type hidden_dim = usize;
pub type output_dim = usize; 


#[derive(Debug , Clone)]
pub struct AttentionHead {
    pub input_dim: usize,
    pub output_dim: usize,
    pub weight_matrix: Vec<Vec<f32>>,      
    pub attention_vector: Vec<f32>,       
    pub dropout_rate: f32,
}

impl AttentionHead {
    pub fn new(input_dim: usize, output_dim: usize, dropout_rate: f32) -> Self {
        let mut rng = rand::thread_rng();
        
        // Initialize weight matrix with Xavier initialization
        let xavier_std = (2.0 / (input_dim + output_dim) as f32).sqrt();
        let weight_matrix = (0..output_dim)
            .map(|_| {
                (0..input_dim)
                    .map(|_| rng.gen_range(-xavier_std..xavier_std))
                    .collect()
            })
            .collect();
        
        // Initialize attention vector
        let attention_vector = (0..2 * output_dim)
            .map(|_| rng.gen_range(-0.1..0.1))
            .collect();
        
        AttentionHead {
            input_dim,
            output_dim,
            weight_matrix,
            attention_vector,
            dropout_rate,
        }
    }
    
    // Linear transformation: h' = W * h
    fn linear_transform(&self, features: &[f32]) -> Vec<f32> {
        self.weight_matrix
            .iter()
            .map(|row| {
                row.iter().zip(features.iter()).map(|(w, f)| w * f).sum()
            })
            .collect()
    }
    
    fn compute_attention_coef(&self, features_i: &[f32], features_j: &[f32]) -> f32 {
        let h_i = self.linear_transform(features_i);
        let h_j = self.linear_transform(features_j);
        
        let concat_features: Vec<f32> = h_i.into_iter().chain(h_j.into_iter()).collect();
        
        let attention_score: f32 = self.attention_vector
            .iter()
            .zip(concat_features.iter())
            .map(|(a, h)| a * h)
            .sum();
        
        self.leaky_relu(attention_score, 0.2)
    }
    
    fn forward(&self , graph: &Graph , node_id: NodeID) -> Vec<f32>{
        let node =  &graph.nodes.get(node_id);
        let node_features = &node.features;
        
        let node_neighbours = GraphOps::neighbors(node_id , direction);
        
        let mut all_neighbours = node_neighbours; 
        all_neighbours.push(node_id);
        
        let attn_coefs = all_neighbours
            .iter()
            .map(|&neighbour_id| {
                let neighbour_features = &graph.nodes.get(neighbour_id).features;
                self.compute_attention_coef(node_features, neighbour_features)
            })
            .collect();
        
        let attn_softmax = softmax(&attn_coefs)?;
        
        let mut output = vec![0.0; self.output_dim];
        for (i, &neighbor_id) in all_neighbors.iter().enumerate() {
            let neighbor_features = &graph.node_features[&neighbor_id];
            let transformed_features = self.linear_transform(neighbor_features);
            let weight = self.apply_dropout(attention_weights[i]);
            
            for j in 0..self.output_dim {
                output[j] += weight * transformed_features[j];
            }
        }
        
        output
        
    }

}

pub struct GATLayer{
    pub embedding_dim : embedding_dim,
    pub input_dim : input_dim,
    pub hidden_dim : hidden_dim,
    pub output_dim : output_dim,
    pub attention_matrix : Vec<Vec<Tensor>>,
    pub num_heads : usize, 
    pub trans_blocks : Vec<AttentionBlock>,
    pub weights_per_head : Vec<num_heads>,
    pub is_weighted: bool,
    pub is_concat : bool, 
}

impl GATLayer{ 
    
    pub fn new(embedding_dim : embedding_dim , input_dim : input_dim , hidden_dim : hidden_dim , output_dim : output_dim , num_heads : usize) -> Self{
        let attention_matrix = vec![vec![Tensor::zeros([embedding_dim , embedding_dim] , Device::Cpu) ; num_heads] ; num_heads];
        Self{
            embedding_dim,
            input_dim,
            hidden_dim,
            output_dim,
            attention_matrix,
            num_heads,
            trans_blocks: Vec::new(),
        }
    }
    
    pub fn forward(&mut self , graph: Graph) -> Vec<f32>{
        let head_outputs : Vec<Vec<f32>> = 
            self.heads
                .iter()
                .map(|head| head.forward(graph , node_id))
                .collect();
        
        if self.is_concat { 
            head_outputs.into_iter().flatten().collect()
        }
        
        else if is_weighted {
            let weight = self.weights_per_head;
            let weighted_outputs = head_outputs
                .into_iter()
                .map(|head_output|
                    weight.into_iter()
                        .zip(head_output)
                        .map(|(weight, value)| weight * value)
                        .collect()
                )
                .collect();
            weighted_outputs.into_iter().flatten().collect()
        }
        
        else if is_averaged{
        
            let output_dim = head_outputs[0].len();
            let mut averaged_output = vec![0.0; output_dim];
        
            for head_output in &head_outputs {
                for (i, &value) in head_output.iter().enumerate() {
                    averaged_output[i] += value;
                }
            }
        
            averaged_output
                .into_iter()
                .map(|sum| sum / self.heads.len() as f32)
                .collect()
        }
        
        else {
            head_outputs.into_iter().flatten().collect()
        }
    }
}

pub struct GAT{
    pub layers: Vec<GATLayer> 
}

impl GAT{ 
    pub fn new() -> Self {
        GAT{layers: Vec::new()}
    }
    
    pub fn add_layer(&mut self , layer:GATLayer){
        self.layers.push(layer);
    }
    
    fn elu(&self, x: f32, alpha: f32) -> f32 {
        if x > 0.0 {
            x
        } else {
            alpha * (x.exp() - 1.0)
        }
    }
    
    pub async fn forward(&self, graph: &Graph) -> HashMap<usize, Vec<f32>> {
        let mut current_features = HashMap::new();
        
        for &node_id in &graph.nodes {
            let features = graph.get_encoded_features(node_id).await;
            current_features.insert(node_id, features);
        }
        v
        for (layer_idx, layer) in self.layers.iter().enumerate() {
            let layer_outputs = futures::future::join_all(
                graph.nodes.iter().map(|&node_id| async {
                    let mut output = layer.forward(graph, node_id).await;
                    
                    // Apply activation (except last layer)
                    if layer_idx < self.layers.len() - 1 {
                        output = output
                            .into_iter()
                            .map(|x| self.elu(x, 1.0))
                            .collect();
                    }
                    
                    (node_id, output)
                })
            ).await;
            
            current_features = layer_outputs.into_iter().collect();
        }
        
        current_features
    }
}
            