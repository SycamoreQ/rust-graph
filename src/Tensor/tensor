///tensor related/// 
use std::collections::HashMap;
use std::sync::{Arc, Mutex};
use std::fmt;
use rayon::prelude::*;

/// Device abstraction for different compute backends
#[derive(Debug, Clone, PartialEq)]
pub enum Device {
    CPU,
    CUDA(u32),  // GPU device ID
    Metal,      // Apple Metal
}

impl Default for Device {
    fn default() -> Self {
        Device::CPU
    }
}

/// Data types supported by the tensor system
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum DType {
    F32,
    F64,
    I32,
    I64,
    U32,
    Bool,
}

impl DType {
    pub fn size_in_bytes(&self) -> usize {
        match self {
            DType::F32 => 4,
            DType::F64 => 8,
            DType::I32 => 4,
            DType::I64 => 8,
            DType::U32 => 4,
            DType::Bool => 1,
        }
    }
}

/// Storage backend for tensor data
#[derive(Debug)]
pub enum Storage {
    CPU(Vec<u8>),
    CUDA { ptr: *mut u8, size: usize, device_id: u32 },
    Metal { buffer: Vec<u8> }, // Simplified for demo
}

unsafe impl Send for Storage {}
unsafe impl Sync for Storage {}

impl Storage {
    pub fn new_cpu(size: usize) -> Self {
        Storage::CPU(vec![0u8; size])
    }

    pub fn size(&self) -> usize {
        match self {
            Storage::CPU(data) => data.len(),
            Storage::CUDA { size, .. } => *size,
            Storage::Metal { buffer } => buffer.len(),
        }
    }

    pub fn device(&self) -> Device {
        match self {
            Storage::CPU(_) => Device::CPU,
            Storage::CUDA { device_id, .. } => Device::CUDA(*device_id),
            Storage::Metal { .. } => Device::Metal,
        }
    }
}

#[derive(Debug , Clone , PartialEq)]
pub struct Layout{
    pub shape : vec<usize>,
    pub stride : vec<usize>,
    pub offset : vec<usize>
}

impl Layout {
    pub fn new(shape: Vec<usize>) -> Self {
        let mut strides = vec![1; shape.len()];
        for i in (0..shape.len().saturating_sub(1)).rev() {
            strides[i] = strides[i + 1] * shape[i + 1];
        }
        Layout { shape, strides, offset: 0 }
    }

    pub fn contiguous(shape: Vec<usize>) -> Self {
        Self::new(shape)
    }

    pub fn numel(&self) -> usize {
        self.shape.iter().product()
    }

    pub fn ndim(&self) -> usize {
        self.shape.len()
    }

    pub fn is_contiguous(&self) -> bool {
        let mut expected_stride = 1;
        for i in (0..self.shape.len()).rev() {
            if self.strides[i] != expected_stride {
                return false;
            }
            expected_stride *= self.shape[i];
        }
        true
    }

    pub fn transpose(&self, dim0: usize, dim1: usize) -> Layout {
        let mut new_shape = self.shape.clone();
        let mut new_strides = self.strides.clone();
        new_shape.swap(dim0, dim1);
        new_strides.swap(dim0, dim1);
        Layout {
            shape: new_shape,
            strides: new_strides,
            offset: self.offset,
        }
    }

    pub fn reshape(&self, new_shape: Vec<usize>) -> Result<Layout, TensorError> {
        let old_numel = self.numel();
        let new_numel: usize = new_shape.iter().product();
        
        if old_numel != new_numel {
            return Err(TensorError::ShapeMismatch(
                format!("Cannot reshape tensor of size {} to size {}", old_numel, new_numel)
            ));
        }

        if !self.is_contiguous() {
            return Err(TensorError::LayoutError("Can only reshape contiguous tensors".to_string()));
        }

        Ok(Layout::contiguous(new_shape))
    }
}

#[derive(Debug)]
pub enum TensorError {
    ShapeMismatch(String),
    DeviceMismatch(String),
    DTypeMismatch(String),
    LayoutError(String),
    ComputeError(String),
    OutOfMemory(String),
}

impl fmt::Display for TensorError {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            TensorError::ShapeMismatch(msg) => write!(f, "Shape mismatch: {}", msg),
            TensorError::DeviceMismatch(msg) => write!(f, "Device mismatch: {}", msg),
            TensorError::DTypeMismatch(msg) => write!(f, "Data type mismatch: {}", msg),
            TensorError::LayoutError(msg) => write!(f, "Layout error: {}", msg),
            TensorError::ComputeError(msg) => write!(f, "Compute error: {}", msg),
            TensorError::OutOfMemory(msg) => write!(f, "Out of memory: {}", msg),
        }
    }
}

impl std::error::Error for TensorError {}

/// Gradient information for automatic differentiation
#[derive(Debug)]
pub struct GradientInfo {
    pub requires_grad: bool,
    pub grad: Option<Tensor>,
    pub grad_fn: Option<Arc<dyn GradFn>>,
}

impl Default for GradientInfo {
    fn default() -> Self {
        GradientInfo {
            requires_grad: false,
            grad: None,
            grad_fn: None,
        }
    }
}

/// Trait for gradient computation functions
pub trait GradFn: Send + Sync {
    fn backward(&self, grad_output: &Tensor) -> Vec<Tensor>;
    fn name(&self) -> &str;
}

pub struct Tensor {
    storage : Arc<Mutex<Storage>>,
    layout : Layout ,
    gradient_info : GradientInfo,
    dtype : DType
}

impl Clone for Tensor {
    fn clone(&self) -> Self{
        Tensor{
            storage: Arc::clone(&self.storage),
            layout: self.layout.clone(),
            gradient_info: self.gradient_info.clone(),
            dtype: self.dtype.clone()
        }
    }
}

impl Tensor{
    pub fn new(shape: &[usize] , dtype: Dtype , device: Device) -> Result<self , TensorError> {
        let layout = Layout::contiguous(shape.to_vec());
        let storage = Storage::new(layout.clone(), dtype.clone(), device.clone())?;
        
        let size_bytes = layout.numel() * dtype.size_in_bytes();
        
        let storage = match device {
            Device::CPU => Storage::new_cpu(size_bytes),
            Device::CUDA(device_id) => {
                // In a real implementation, allocate CUDA memory here
                return Err(TensorError::ComputeError("CUDA not implemented in demo".to_string()));
            }
            Device::Metal => {
                // In a real implementation, allocate Metal buffer here
                return Err(TensorError::ComputeError("Metal not implemented in demo".to_string()));
            }
        };

        Ok(Tensor {
            storage: Arc::new(Mutex::new(storage)),
            layout,
            dtype,
            grad_info: None,
        })
    }
    
    pub fn from_data<T: Into<Vec<f32>>>(data: T, shape: &[usize]) -> Result<Self, TensorError> {
         let data_vec: Vec<f32> = data.into();
         let expected_numel: usize = shape.iter().product();
         
         if data_vec.len() != expected_numel {
             return Err(TensorError::ShapeMismatch(
                 format!("Data length {} doesn't match shape {:?}", data_vec.len(), shape)
             ));
         }
 
         let layout = Layout::contiguous(shape.to_vec());
         let size_bytes = layout.numel() * DType::F32.size_in_bytes();
         let mut storage_data = vec![0u8; size_bytes];
         
         // Copy f32 data to u8 storage
         let f32_slice = unsafe {
             std::slice::from_raw_parts_mut(
                 storage_data.as_mut_ptr() as *mut f32,
                 data_vec.len()
             )
         };
         f32_slice.copy_from_slice(&data_vec);
 
         Ok(Tensor {
             storage: Arc::new(Mutex::new(Storage::CPU(storage_data))),
             layout,
             dtype: DType::F32,
             grad_info: None,
         })
     }
     
     pub fn zeros(shape: &[usize] , dtype: Dtype , device: Device) -> Result<Self , TensorError>{
         Self::new(shape , dtype , device)
     }
     
     pub fn ones(shape : &[usize] , dtype : Dtype , device: Device) -> Result<Self , TensorError>{
         let tensor = Self::new(shape , dtype , device)?;
         tensor.fill(1.0);
         Ok(tensor)
     }
     
     pub fn 
     
    }
}
